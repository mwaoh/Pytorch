{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP6ygkNWdnjQuF5ZwBkH01y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mwaoh/Pytorch/blob/main/Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "clJREI7QyfwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('')\n",
        "\n",
        "#from google.colab import drive\n",
        "#uploades = files.upload()\n",
        "#df = pd.read_csv(\"zomato.csv\", encoding = \"latin-1\")\n",
        "\n",
        "# Read data from file\n",
        "data = pd.read_csv(\"data.csv\")\n",
        "\n",
        "# Print the first 5 rows of the data\n",
        "print(data.head())\n",
        "\n",
        "# Visualize data with a scatter plot\n",
        "plt.scatter(data['Feature1'], data['Feature2'])\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('Scatter plot of Feature 1 vs. Feature 2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uLBR23vy2_0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data\n",
        "data = np.array([['apple', 'red', 'round'],\n",
        "                 ['banana', 'yellow', 'curved'],\n",
        "                 ['orange', 'orange', 'round']])\n",
        "\n",
        "# One-hot encoding\n",
        "vocab = np.unique(data) # Create vocabulary of unique values\n",
        "onehot = np.zeros((data.shape[0], len(vocab))) # Initialize one-hot matrix\n",
        "for i, row in enumerate(data):\n",
        "    for j, item in enumerate(row):\n",
        "        onehot[i, np.where(vocab == item)[0]] = 1 # Set one-hot value to 1 for matching vocabulary index\n",
        "print(\"One-hot matrix:\\n\", onehot)\n",
        "\n",
        "# Embedding\n",
        "embedding_dim = 3 # Embedding dimension\n",
        "embedding = nn.Embedding(len(vocab), embedding_dim) # Initialize embedding layer\n",
        "inputs = torch.tensor([np.where(vocab == item)[0][0] for item in data.flatten()], dtype=torch.long) # Convert data to indices\n",
        "inputs = inputs.view(data.shape[0], -1) # Reshape indices to match original data shape\n",
        "embedded = embedding(inputs) # Embed inputs\n",
        "print(\"Embedded matrix:\\n\", embedded.detach().numpy())\n"
      ],
      "metadata": {
        "id": "SQxsZNm72Vdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input):\n",
        "        embedded = self.embedding(input)\n",
        "        output, hidden = self.lstm(embedded)\n",
        "        return output, hidden\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_size, hidden_size):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input)\n",
        "        output, hidden = self.lstm(embedded, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        enc_output, enc_hidden = self.encoder(input)\n",
        "        dec_hidden = enc_hidden\n",
        "        dec_input = torch.tensor([[SOS_token]], device=device)\n",
        "        dec_outputs = torch.zeros(target.size(0), target.size(1), self.decoder.output_size, device=device)\n",
        "        for t in range(target.size(1)):\n",
        "            dec_output, dec_hidden = self.decoder(dec_input, dec_hidden)\n",
        "            dec_outputs[:, t, :] = dec_output\n",
        "            dec_input = target[:, t].unsqueeze(1)\n",
        "        return dec_outputs\n"
      ],
      "metadata": {
        "id": "rBsqu77N3kJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your GAN architecture using PyTorch\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        # Define your generator architecture using PyTorch layers\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Define the forward pass for your generator\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        # Define your discriminator architecture using PyTorch layers\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Define the forward pass for your discriminator\n",
        "\n",
        "# Define your loss function and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "# Define your training data and data loader\n",
        "class POIDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        # Load and preprocess your POI data\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        # Return a sample from your POI data\n",
        "    \n",
        "dataset = POIDataset()\n",
        "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Train your GAN on your POI data\n",
        "for epoch in range(num_epochs):\n",
        "    for i, data in enumerate(dataloader):\n",
        "        # Train your generator and discriminator using PyTorch\n",
        "        # Update the weights of your networks using the optimizer\n",
        "    \n",
        "# Use your trained GAN to generate new POI information\n",
        "location = [\n"
      ],
      "metadata": {
        "id": "3qi_BB8KPd2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "# Define the discriminator network\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        out = self.linear(out[:, -1, :])\n",
        "        out = torch.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "# Define the generator network\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_size):\n",
        "        super(Generator, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "# Define the loss function\n",
        "def gail_loss(expert_seq, generated_seq, discriminator):\n",
        "    expert_labels = torch.ones(expert_seq.shape[0], 1).cuda()\n",
        "    generated_labels = torch.zeros(generated_seq.shape[0], 1).cuda()\n",
        "    expert_preds = discriminator(expert_seq)\n",
        "    generated_preds = discriminator(generated_seq)\n",
        "    adversarial_loss = nn.BCELoss()(generated_preds, expert_labels)\n",
        "    imitation_loss = nn.MSELoss()(generated_seq, expert_seq)\n",
        "    total_loss = adversarial_loss + imitation_loss\n",
        "    return total_loss\n",
        "\n",
        "# Define the hyperparameters\n",
        "input_size = 10\n",
        "output_size = 5\n",
        "hidden_size = 64\n",
        "batch_size = 32\n",
        "num_epochs = 1000\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Initialize the discriminator and generator networks\n",
        "discriminator = Discriminator(input_size + output_size, hidden_size).cuda()\n",
        "generator = Generator(input_size, output_size, hidden_size).cuda()\n",
        "\n",
        "# Define the optimizer and learning rate scheduler\n",
        "discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
        "generator_optimizer = optim.Adam(generator.parameters\n"
      ],
      "metadata": {
        "id": "JuWYaZvS39HO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the necessary libraries and data\n",
        "import numpy as np\n",
        "from keras.models import load_model\n",
        "from preprocess import load_data\n",
        "\n",
        "# Load the trained generator model\n",
        "generator = load_model('generator_model.h5')\n",
        "\n",
        "# Load the data used to train the GAN\n",
        "data = load_data('poi_data.csv')\n",
        "\n",
        "# Define the size of the latent space vector used to generate new POI information\n",
        "latent_dim = 100\n",
        "\n",
        "# Generate a random vector in the latent space\n",
        "location = np.random.randn(1, latent_dim)\n",
        "\n",
        "# Use the generator model to generate new POI information from the random vector\n",
        "generated_poi_info = generator.predict(location)\n",
        "\n",
        "# Print the generated POI information\n",
        "print(generated_poi_info)\n"
      ],
      "metadata": {
        "id": "M9LZt6I7RBmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "# Define the LSTM-based sequence generator\n",
        "class LSTMGenerator(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(LSTMGenerator, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "    \n",
        "    def forward(self, input, hidden):\n",
        "        output, hidden = self.lstm(input, hidden)\n",
        "        output = self.out(output.view(-1, self.hidden_size))\n",
        "        return output, hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        return (torch.zeros(1, batch_size, self.hidden_size),\n",
        "                torch.zeros(1, batch_size, self.hidden_size))\n",
        "\n",
        "# Define the CNN-based discriminator\n",
        "class CNNDiscriminator(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size, filter_sizes, num_filters):\n",
        "        super(CNNDiscriminator, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv2d(1, num_filters, (f, embedding_size)) for f in filter_sizes\n",
        "        ])\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc = nn.Linear(num_filters * len(filter_sizes), 1)\n",
        "    \n",
        "    def forward(self, input):\n",
        "        input = self.embedding(input)\n",
        "        input = input.unsqueeze(1)\n",
        "        conv_outputs = []\n",
        "        for conv in self.convs:\n",
        "            conv_output = conv(input)\n",
        "            conv_output = nn.functional.relu(conv_output.squeeze(3))\n",
        "            pool_output = nn.functional.max_pool1d(conv_output, conv_output.size(2)).squeeze(2)\n",
        "            conv_outputs.append(pool_output)\n",
        "        output = torch.cat(conv_outputs, 1)\n",
        "        output = self.dropout(output)\n",
        "        output = self.fc(output)\n",
        "        return output.squeeze()\n",
        "\n",
        "# Define the SeqGAN model\n",
        "class SeqGAN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, vocab_size, embedding_size, filter_sizes, num_filters, learning_rate):\n",
        "        super(SeqGAN, self).__init__()\n",
        "        self.generator = LSTMGenerator(input_size, hidden_size, output_size)\n",
        "        self.discriminator = CNNDiscriminator(vocab_size, embedding_size, filter_sizes, num_filters)\n",
        "        self.optimizer_G = optim.Adam(self.generator.parameters(), lr=learning_rate)\n",
        "        self.optimizer_D = optim.Adam(self.discriminator.parameters(), lr=learning_rate)\n",
        "        self.criterion = nn.BCEWithLogitsLoss()\n",
        "    \n",
        "    def generate(self, input, length):\n",
        "        batch_size = input.size(1)\n",
        "        hidden = self.generator.init_hidden(batch_size)\n",
        "        output_seq = []\n",
        "        for i in range(length):\n",
        "            output, hidden = self.generator(input, hidden)\n",
        "            output_seq.append(output)\n",
        "            input = output.detach()\n",
        "        output_seq = torch.cat(output_seq, dim=0)\n",
        "        return output_seq\n",
        "    \n",
        "    def train_G(self, input, target, reward):\n",
        "        batch_size = input.size(1)\n",
        "        hidden = self.generator.init_hidden(batch_size)\n",
        "        output_seq = []\n",
        "        for i in range(target.size(0)):\n",
        "            output, hidden = self.generator(input, hidden)\n",
        "            output_seq.append(output)\n",
        "            input = target[i].unsqueeze(0)\n",
        "        output_seq = torch.cat(output_seq, dim=0)\n",
        "        loss = torch.sum(-reward * output_seq) / batch_size\n",
        "        self.optimizer_G.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer_G.step()\n",
        "    \n",
        "    def\n"
      ],
      "metadata": {
        "id": "ssIv2UOhav-U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}